LCE Presentation
================
**Local Cascade Ensemble (LCE)** proposes to answer the recurrent question faced by data science practitioners: **Random Forest or XGBoost**?

LCE combines their strengths and adopts a complementary diversification approach to obtain a better generalizing predictor. 
Thus, LCE further enhances the prediction performance of both Random Forest and XGBoost.

Overview
--------
The construction of an ensemble method involves combining accurate and diverse individual predictors. 
There are **two complementary ways** to generate diverse predictors: *(i)* by **changing the training data distribution** and *(ii)* by **learning different parts of the training data**.

**LCE adopts these two diversification approaches.** 
First, *(i)* LCE combines the two well-known methods that modify the distribution of the original training data with complementary effects on the bias-variance trade-off: bagging [1]_ (variance reduction) and boosting [2]_ (bias reduction). 
Then, *(ii)* LCE learns different parts of the training data to capture new relationships that cannot be discovered globally based on a divide-and-conquer strategy (a decision tree). 
Before detailing how LCE combines these methods, we introduce the key concepts behind them that will be used in the explanation of LCE.

Concepts
--------
The bias-variance trade-off defines the capacity of the learning algorithm to generalize beyond the training set. 
The *bias* is the component of the prediction error that results from systematic errors of the learning algorithm. 
A high bias means that the learning algorithm is not able to capture the underlying structure of the training set (underfitting). 
The *variance* measures the sensitivity of the learning algorithm to changes in the training set. 
A high variance means that the algorithm is learning too closely the training set (overfitting). 
The objective is to minimize both the bias and variance. *Bagging* has a main effect on variance reduction; it is a method for generating multiple versions of a predictor (bootstrap replicates) and using these to get an aggregated predictor. 
The current state-of-the-art method that employs bagging is Random Forest [3]_. 
Whereas, *boosting* has a main effect on bias reduction; it is a method for iteratively learning weak predictors and adding them to create a final strong one. 
After a weak learner is added, the data weights are readjusted, allowing future weak learners to focus more on the examples that previous weak learners mispredicted. 
The current state-of-the-art method that uses boosting is XGBoost [4]_. 
The following Figure illustrates the difference between bagging and boosting methods.

.. raw:: html

	<p align="center">
		<img src="./_images/Figure_BaggingvsBoosting.png" width="90%">	
	</p>

LCE 
---
LCE combines a boosting-bagging approach to handle the bias-variance trade-off faced by machine learning models; in addition, it adopts a divide-and-conquer approach to individualize predictor errors on different parts of the training data. 
LCE is represented in the following Figure.

.. raw:: html

	<p align="center">
		<img src="./_images/Figure_LCE.png" width="90%">	
	</p>


Specifically, LCE is based on cascade generalization: it uses a set of predictors sequentially, and adds new attributes to the input dataset at each stage. 
The new attributes are derived from the output given by a predictor (e.g., class probabilities for a classifier), called a base learner. 
LCE applies cascade generalization locally following a divide-and-conquer strategy - a decision tree, and reduces bias across a decision tree through the use of boosting-based predictors as base learners. 
The current best performing state-of-the-art boosting algorithm is adopted as base learner (XGBoost, e.g., XGB¹°, XGB¹¹ in above Figure). 
When growing the tree, boosting is propagated down the tree by adding the output of the base learner at each decision node as new attributes to the dataset (e.g., XGB¹°(D¹) in above Figure). 
Prediction outputs indicate the ability of the base learner to correctly predict a sample. 
At the next tree level, the outputs added to the dataset are exploited by the base learner as a weighting scheme to focus more on previously mispredicted samples. 
Then, the overfitting generated by the boosted decision tree is mitigated by the use of bagging. 
Bagging provides variance reduction by creating multiple predictors from random sampling with replacement of the original dataset (e.g., D¹, D² in above Figure). 
Finally, trees are aggregated with a simple majority vote. 
In order to be applied as a predictor, LCE stores, in each node, the model generated by the base learner.

Missing Data
------------
LCE natively handles missing data. 
Similar to XGBoost, LCE excludes missing values for the split and uses block propagation. 
During a node split, block propagation sends all samples with missing data to the side of the decision node with less errors.

Hyperparameters
---------------
The hyperparameters of LCE are the classical ones in tree-based learning (e.g., ``max_depth``, ``max_features``, ``n_estimators``). 
Moreover, LCE learns a specific XGBoost model at each node of a tree, and it only requires the ranges of XGBoost hyperparameters to be specified. 
Then, the hyperparameters of each XGBoost model are automatically set by Hyperopt [5]_, a sequential model-based optimization using a tree of Parzen estimators algorithm. 
Hyperopt chooses the next hyperparameters from both the previous choices and a tree-based optimization algorithm. 
Tree of Parzen estimators meet or exceed grid search and random search performance for hyperparameters setting. 
The full list of LCE hyperparameters is available in its :ref:`API documentation <APIDocumentation>`.

Published Results
-----------------
LCE has been initially designed for a specific application in [6]_, and then evaluated it on the public UCI datasets [7]_ in [8]_. 
Results show that LCE obtains on average a better prediction performance than the state-of-the-art classifiers, including Random Forest and XGBoost.

References
----------
.. [1] Breiman, L. Bagging Predictors. Machine Learning, 24(2):123–140, 1996
.. [2] Schapire, R. The Strength of Weak Learnability. Machine Learning, 5(2):197–227, 1990
.. [3] Breiman, L. Random Forests. Machine Learning, 45(1):5–32, 2001
.. [4] Chen, T. and C. Guestrin. XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016
.. [5] Bergstra, J., R. Bardenet, Y. Bengio and B. Kégl. Algorithms for Hyper-Parameter Optimization. In Proceedings of the 24th International Conference on Neural Information Processing Systems, 2011
.. [6] Fauvel, K., V. Masson, E. Fromont, P. Faverdin and A. Termier. Towards Sustainable Dairy Management - A Machine Learning Enhanced Method for Estrus Detection. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019
.. [7] Dua, D. and C. Graff. UCI Machine Learning Repository, 2017
.. [8] Fauvel, K., E. Fromont, V. Masson, P. Faverdin and A. Termier. XEM: An Explainable-by-Design Ensemble Method for Multivariate Time Series Classification. Data Mining and Knowledge Discovery, 36(3):917–957, 2022


Installation
============

You can install LCE from `PyPI <https://pypi.org/project/lcensemble/>`_ with ``pip``::

	pip install lcensemble
	
Or ``conda``::

	conda install -c conda-forge lcensemble


Code Examples
=============

The following examples illustrate the use of LCE on public datasets for a classification and a regression task.
They also demonstrates the compatibility of LCE with scikit-learn model pipelines and selection tools through the use of ``cross_val_score`` and ``GridSearchCV``.
An example of LCE on a dataset including missing values is also shown.

Classification
--------------

- **Example 1: LCE on Iris Dataset**

.. code-block:: python

	from lce import LCEClassifier
	from sklearn.datasets import load_iris
	from sklearn.metrics import classification_report
	from sklearn.model_selection import train_test_split


	# Load data and generate a train/test split
	data = load_iris()
	X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0)

	# Train LCEClassifier with default parameters
	clf = LCEClassifier(n_jobs=-1, random_state=0)
	clf.fit(X_train, y_train)

	# Make prediction and compute accuracy score
	y_pred = clf.predict(X_test)
	accuracy = accuracy_score(y_test, y_pred)
	print("Accuracy: {:.1f}%".format(accuracy*100))
   
.. code-block::
	
	Accuracy: 97.4%


- **Example 2: LCE with scikit-learn cross validation score**
This example demonstrates the compatibility of LCE with scikit-learn model selection tools through the use of ``cross_val_score``.

.. code-block:: python

	from lce import LCEClassifier
	from sklearn.datasets import load_iris
	from sklearn.model_selection import cross_val_score, train_test_split

	# Load data
	data = load_iris()
	X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0)

	# Set LCEClassifier with default parameters
	clf = LCEClassifier(n_jobs=-1, random_state=0)

	# Compute cross-validation scores
	cv_scores = cross_val_score(clf, X_train, y_train, cv=3)
	cv_scores = [round(elem*100, 1) for elem in cv_scores.tolist()]
	print("Cross-validation scores on train set: ", cv_scores)
   
.. code-block::
	
	Cross-validation scores on train set:  [92.1, 100.0, 94.6]


- **Example 3: LCE with missing values**
This example illustrates the robustness of LCE to missing values. The Iris train set is modified with 20% of missing values per variable.

.. code-block:: python

	import numpy as np
	from lce import LCEClassifier
	from sklearn.datasets import load_iris
	from sklearn.metrics import accuracy_score
	from sklearn.model_selection import train_test_split


	# Load data and generate a train/test split
	data = load_iris()
	X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0)

	# Input 20% of missing values per variable in the train set
	np.random.seed(0)
	m = 0.2
	for j in range(0, X_train.shape[1]):
		sub = np.random.choice(X_train.shape[0], int(X_train.shape[0]*m))
		X_train[sub, j] = np.nan

	# Train LCEClassifier with default parameters
	clf = LCEClassifier(n_jobs=-1, random_state=0)
	clf.fit(X_train, y_train)

	# Make prediction and compute accuracy score
	y_pred = clf.predict(X_test)
	accuracy = accuracy_score(y_test, y_pred)
	print("Accuracy: {:.1f}%".format(accuracy*100))


.. code-block::

	Accuracy: 94.7%


Regression
----------

- **Example 4: LCE on Diabetes Dataset**

.. code-block:: python

	from lce import LCERegressor
	from sklearn.datasets import load_diabetes
	from sklearn.metrics import mean_squared_error
	from sklearn.model_selection import train_test_split


	# Load data and generate a train/test split
	data = load_diabetes()
	X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0)

	# Train LCERegressor with default parameters
	reg = LCERegressor(n_jobs=-1, random_state=0)
	reg.fit(X_train, y_train)

	# Make prediction 
	y_pred = reg.predict(X_test)
	mse = mean_squared_error(y_test, reg.predict(X_test))
	print("The mean squared error (MSE) on test set: {:.0f}".format(mse))
	
.. code-block::
	
	The mean squared error (MSE) on test set: 3556


- **Example 5: LCE with scikit-learn best hyperparameter grid search**
This example demonstrates the compatibility of LCE with scikit-learn model selection tools through the use of ``GridSearchCV``.

.. code-block:: python

	from lce import LCERegressor
	from sklearn.datasets import load_diabetes
	from sklearn.model_selection import train_test_split, GridSearchCV


	# Load data and generate a train/test split
	data = load_diabetes()
	X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=0)

	# Build LCERegressor with default parameters
	reg = LCERegressor(n_jobs=-1, random_state=0)

	# Define parameter ranges for grid search
	params = {'n_estimators': list(range(10, 51, 20)),
		  'max_depth': list(range(0, 3, 1))}

	# Run scikit learn grid search 
	grid_cv = GridSearchCV(reg, param_grid=params, cv=3, n_jobs=-1)
	grid_cv.fit(X_train, y_train)

	# Print best configuration
	print("Best n_estimator: ", grid_cv.best_params_['n_estimators'],
	      ", best max_depth: ", grid_cv.best_params_['max_depth'])
	
.. code-block::
	
	Best n_estimator:  30 , best max_depth:  1
	  


Python Source Files
-------------------


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="LCEClassifier on Iris Dataset">

.. only:: html

 .. figure:: ./doc/_images/logo_lce.svg
     :alt: LCEClassifier on Iris dataset

.. raw:: html

    </div>

.. toctree::
   :hidden:

   /auto_examples/lceclassifier_iris
   
   

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="LCEClassifier on Iris Dataset with scikit-learn cross validation score">

.. only:: html

 .. figure:: ./doc/_images/logo_lce.svg
     :alt: LCEClassifier on Iris dataset with scikit-learn cross validation score


.. raw:: html

    </div>

.. toctree::
   :hidden:

   /auto_examples/lceclassifier_iris_cv



.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="LCEClassifier with missing values">

.. only:: html

 .. figure:: ./doc/_images/logo_lce.svg
     :alt: LCEClassifier on Iris dataset with missing values

.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/lceclassifier_missing_iris
   


.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="LCERegressor on Diabetes Dataset">

.. only:: html

 .. figure:: ./doc/_images/logo_lce.svg
     :alt: LCERegressor on Diabetes dataset


.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/lceregressor_diabetes
   
   
.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="LCERegressor on Diabetes Dataset with scikit-learn hyperparameter grid search">

.. only:: html

 .. figure:: ./doc/_images/logo_lce.svg
     :alt: LCERegressor on Diabetes dataset with scikit-learn hyperparameter grid search


.. raw:: html

    </div>


.. toctree::
   :hidden:

   /auto_examples/lceregressor_diabetes_gridsearchcv
   
   

.. raw:: html

    <div class="sphx-glr-clear"></div>



.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-gallery


  .. container:: sphx-glr-download sphx-glr-download-python

    :download:`Download all examples in Python source code: auto_examples_python.zip </auto_examples/auto_examples_python.zip>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

    :download:`Download all examples in Jupyter notebooks: auto_examples_jupyter.zip </auto_examples/auto_examples_jupyter.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
   